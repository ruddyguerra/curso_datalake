# ğŸ§ª Proyecto Data Lake con: MinIO + PySpark + Airflow + Trino + DuckDB

Este proyecto levanta un entorno completo para simular un Data Lake moderno en tu mÃ¡quina local utilizando contenedores Docker.

---

## ğŸ”§ Requisitos

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- VSCode
- Sistema operativo: Windows/macOS/Linux

---

## ğŸ“¦ Â¿QuÃ© contiene este proyecto?

| Servicio | DescripciÃ³n |
|----------|-------------|
| **MinIO** | SimulaciÃ³n local de Amazon S3 |
| **PySpark** | Procesamiento de archivos CSV a Parquet |
| **boto3** | Subida de archivos a MinIO desde Python |
| **DuckDB** | Consultas SQL directas sobre archivos locales |
| **Trino** | Consultas SQL sobre Parquet en S3 (MinIO) |
| **Airflow** | AutomatizaciÃ³n de pipelines con DAGs |

---

## ğŸ“ Estructura

```
curso_datalake/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ data/                    # Archivos CSV de entrada
â”‚   â””â”€â”€ demo.csv
â”œâ”€â”€ scripts/                 # Scripts Python
â”‚   â”œâ”€â”€ upload_csv_to_minio.py
â”‚   â”œâ”€â”€ csv_to_parquet_s3.py
â”‚   â””â”€â”€ query_duckdb.py
â”œâ”€â”€ dags/                   # DAG de Airflow
â”‚   â””â”€â”€ csv_to_parquet_dag.py
â””â”€â”€ trino/etc/catalog/      # ConfiguraciÃ³n del conector Hive para Trino
    â””â”€â”€ hive.properties
```

---

## ğŸš€ Instrucciones de uso

### 1. Levanta los contenedores

```bash
docker-compose up -d --build
```

Para reconstruir

```bash
docker-compose build app
docker-compose up -d
```

Para validar los procesos

```bash
docker ps
docker inspect app
```

Se iniciarÃ¡n los servicios en:

- MinIO â†’ [http://localhost:9001](http://localhost:9001)  
  Usuario: `minioadmin`, ContraseÃ±a: `minioadmin`
- Airflow â†’ [http://localhost:8080](http://localhost:8080)  
  Usuario: `admin`, ContraseÃ±a: `admin`
- Trino â†’ [http://localhost:8081](http://localhost:8081)

---

### 2. Ejecuta los scripts manuales

```bash
docker exec -it app bash
```

Dentro del contenedor, ejecuta en orden:

```bash
python scripts/upload_csv_to_minio.py
python scripts/csv_to_parquet_s3.py
python scripts/query_duckdb.py
```

---

### 3. Verifica resultados

#### âœ… En MinIO

- Bucket: `demo-bucket`
- Archivos esperados:
  - `demo.csv`
  - Carpeta `output_parquet/` con archivos Parquet

#### âœ… En Airflow

1. Entra al UI: [http://localhost:8080](http://localhost:8080)
2. Activa y ejecuta el DAG: `csv_to_parquet_pipeline`
3. Visualiza los logs de cada tarea

#### âœ… En Trino

```bash
docker-compose up -d trino
```

```bash
docker exec -it trino trino
```

Y ejecuta:

```sql
SHOW CATALOGS;
SHOW SCHEMAS FROM hive;
SHOW TABLES FROM hive."demo-bucket";
SELECT * FROM hive."demo-bucket"."output_parquet" LIMIT 10;
```

#### âœ… En DuckDB

Consulta local sobre archivos CSV:

```bash
python scripts/query_duckdb.py
```

---

## âœ¨ Extras

- Puedes agregar mÃ¡s archivos CSV a la carpeta `data/` para probar.

---

## ğŸ“¬ Autor

Preparado por Ruddy Guerra para entrenamiento sobre Data Lakes modernos con herramientas de cÃ³digo abierto y Python.